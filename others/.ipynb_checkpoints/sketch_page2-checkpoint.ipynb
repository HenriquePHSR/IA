{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datalib_mt5 as dlm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlt\n",
    "import pickle\n",
    "import time\n",
    "import sqlite3\n",
    "import tensorflow as tf\n",
    "import thread_main_loop as tml\n",
    "import thread_training\n",
    "import queue\n",
    "import threading\n",
    "import struct\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_columns(dataset, shift, fulltime):\n",
    "    x_Mn = dataset\n",
    "    x_Mn['t_DATE-TIME'] = x_Mn['t_DATE'] + x_Mn['t_TIME']\n",
    "    x_Mn = x_Mn.drop_duplicates(subset=['t_DATE-TIME'])\n",
    "    x_Mn = x_Mn.set_index(np.arange(len(x_Mn)))\n",
    "    x_Mn['t_LAST_Shift'] = x_Mn['t_LAST'].shift(-shift)\n",
    "    x_Mn['t_TIME_Shift'] = x_Mn['t_TIME'].shift(-shift)\n",
    "    x_Mn['t_LAST_DELTA'] = x_Mn['t_LAST_Shift'] - x_Mn['t_LAST']\n",
    "    x_Mn = x_Mn.dropna()\n",
    "\n",
    "    for index, row in x_Mn.iterrows():\n",
    "        if row['t_LAST_DELTA'] > target:\n",
    "            x_Mn.loc[index,'t_POS_up'] = 1\n",
    "            x_Mn.loc[index,'t_POS_down'] = 0\n",
    "            x_Mn.loc[index,'t_POS_const'] = 0\n",
    "        elif row['t_LAST_DELTA'] < -target:\n",
    "            x_Mn.loc[index,'t_POS_down'] = 1\n",
    "            x_Mn.loc[index,'t_POS_up'] = 0\n",
    "            x_Mn.loc[index,'t_POS_const'] = 0\n",
    "        else:\n",
    "            x_Mn.loc[index,'t_POS_const'] = 1\n",
    "            x_Mn.loc[index,'t_POS_up'] = 0\n",
    "            x_Mn.loc[index,'t_POS_down'] = 0\n",
    "    if !fulltime:\n",
    "        x_Mn=x_Mn[x_Mn['t_TIME']<'14:00']\n",
    "        x_Mn=x_Mn.set_index(np.arange(len(x_Mn)))\n",
    "    return x_Mn\n",
    "\n",
    "def process_dataset(dataset, is_categorical, shift, target, feature_list, x_matrix_size, fulltime)\n",
    "    if is_categorical:\n",
    "        y = tf.keras.utils.to_categorical(x_Mn[['t_POS_up','t_POS_down','t_POS_const']], num_classes=3)\n",
    "    else:\n",
    "        y = x_Mn['t_LAST_DELTA']\n",
    "        \n",
    "    x=x_Mn[feature_list]\n",
    "    x_matrix = []\n",
    "    y_matrix = y.loc[x_matrix_size:]\n",
    "    y_matrix = pd.DataFrame(y_matrix)\n",
    "    y_values_matrix = []\n",
    "\n",
    "    \n",
    "    for index, row in y_matrix.iterrows():\n",
    "        x_matrix.append(x[index-x_matrix_size:index])\n",
    "        y_values_matrix.append(row.values)\n",
    "    x_values_matrix = []\n",
    "    for element in x_matrix:\n",
    "        x_values_matrix.append(element.values)\n",
    "    x_values_matrix = np.array(x_values_matrix)\n",
    "    y_values_matrix = np.array(y_values_matrix)\n",
    "    \n",
    "    x_processed=x_values_matrix\n",
    "    y_processed=y_values_matrix\n",
    "    \n",
    "    return x_processed, y_processed\n",
    "    \n",
    "def class_lenght(y_processed):\n",
    "    class_count=[]\n",
    "    for i in range(len(y_processed[0])):\n",
    "        class_count.append(len([y[i]>0 for y in y_processed]))\n",
    "    print(class_count)\n",
    "        \n",
    "    \n",
    "def under_sample(x_values_matrix, y_values_matrix):\n",
    "    x_processed=[]\n",
    "    y_processed=[]\n",
    "    aux=0\n",
    "    for i in range(len(y_values_matrix)):\n",
    "        if np.array_equal(y_values_matrix[i],np.array([1,0,0])):\n",
    "            x_processed.append(x_values_matrix[i])\n",
    "            y_processed.append(y_values_matrix[i])\n",
    "        if np.array_equal(y_values_matrix[i],np.array([0,1,0])):\n",
    "            x_processed.append(x_values_matrix[i])\n",
    "            y_processed.append(y_values_matrix[i])\n",
    "        if np.array_equal(y_values_matrix[i],np.array([0,0,1])):\n",
    "            if aux==8:\n",
    "                x_processed.append(x_values_matrix[i])\n",
    "                y_processed.append(y_values_matrix[i])\n",
    "                aux=0\n",
    "            else:\n",
    "                aux+=1\n",
    "                \n",
    "#callbacks = myCallback()\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.6):\n",
    "            print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "#callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "def scheduler(epoch, lr):\n",
    "      if epoch < 10:\n",
    "        return lr\n",
    "      else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "def variation_calc(a_matrix, Weight):\n",
    "    line_lenght=(a_matrix.shape)[-2]-1\n",
    "    variation_matrix=[]\n",
    "    while line_lenght>0:\n",
    "        i=0\n",
    "        variation_line=[]\n",
    "        for x in a_matrix[line_lenght]:\n",
    "            var= (((abs(a_matrix[line_lenght][i])/abs(a_matrix[line_lenght-1][i]))-1)) * Weight\n",
    "            variation_line.append(var)\n",
    "            i+=1\n",
    "        variation_matrix.append(variation_line)\n",
    "        line_lenght-=1\n",
    "    \n",
    "    return variation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlm.insert_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dlm.request_table()\n",
    "dataset.t_DATE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_feature_list = ['t_BID','t_ASK','t_LAST','iAC','iAD','iADX','iADXWilder','iAlligator','iAMA','iAO','iATR','iBearsPower','iBands','iBullsPower','iCCI','iChaikin','iDEMA','iDeMarker','iEnvelopes','iForce','iFractals','iFrAMA','iIchimoku','iBWMFI','iMomentum','iMFI','iMA','iOsMA','iMACD','iOBV','iSAR','iRSI','iRVI','iStdDEV','iStochastic','iTEMA','iTriX','iWPR','iVIDyA','iVolumes']\n",
    "shift = 10\n",
    "target = 10\n",
    "feature_list = ['iBands','iBullsPower','iCCI','iChaikin','iDEMA','iDeMarker','iEnvelopes','iForce','iFrAMA']\n",
    "tim_step = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=y_columns(dataset, shift, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split =int(len(x_processed)*0.7)\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X, test_X, train_y, test_y = train_test_split(x_processed, y_processed, random_state = 0)\n",
    "train_X, test_X, train_y, test_y = x_processed[:split], x_processed[split:], y_processed[:split], y_processed[split:]\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.squeeze(train_y,axis=-1)\n",
    "test_y=np.squeeze(test_y,axis=-1)\n",
    "scaler = MinMaxScaler()\n",
    "test_y_scaled = scaler.fit_transform(test_y.reshape(-1,1))\n",
    "train_y_scaled = scaler.fit_transform(train_y.reshape(-1,1))\n",
    "test_y_scaled = np.squeeze(test_y_scaled,axis=-1)\n",
    "train_y_scaled = np.squeeze(train_y_scaled,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE('minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_squeeze=train_X.reshape(len(train_X),len(train_X[0]) * len(train_X[0][0]))\n",
    "print(train_X_squeeze.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer = tf.keras.layers.LayerNormalization(axis=1)\n",
    "#output = layer(train_X[rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_os, train_y_os = smote.fit_sample(train_X_squeeze, train_y)\n",
    "print(train_X_os.shape, train_y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_os, train_y_os = smote.fit_sample(train_X_os, train_y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X_os.shape, train_y_os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up=0\n",
    "down=0\n",
    "const=0\n",
    "for i in range(len(train_y_os)):\n",
    "    if np.array_equal(train_y_os[i],np.array([1,0,0])):\n",
    "        up+=1\n",
    "    if np.array_equal(train_y_os[i],np.array([0,1,0])):\n",
    "        down+=1\n",
    "    if np.array_equal(train_y_os[i],np.array([0,0,1])):\n",
    "        const+=1\n",
    "print(up)\n",
    "print(down)\n",
    "print(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_os = train_X_os.reshape(len(train_X_os), len(train_X[0]), len(train_X[0][0]))\n",
    "print(train_X_os.shape, train_y_os.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_X[index], norm=norm)\n",
    "#norm = mlt.colors.Normalize(vmin=0.,vmax=1.)\n",
    "layer = tf.keras.layers.LayerNormalization(axis=1)\n",
    "fig=plt.figure(figsize=(18,30))\n",
    "for i in range(5):\n",
    "    while (not ready):\n",
    "        rand = random.randint(0, len(train_y)-1)\n",
    "        if np.array_equal(train_y[rand],[1.0, 0., 0.]):\n",
    "            plt.subplot(1,5,i+1)\n",
    "            plt.subplot(1,5,i+1)\n",
    "            \n",
    "            output = layer(train_X[rand])\n",
    "            plt.imshow(output)\n",
    "            #plt.imshow(train_X[rand], norm=norm)\n",
    "            plt.title(str(rand)+' / '+str(train_y[rand]))\n",
    "            ready = True\n",
    "    ready=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_X[index], norm=norm)\n",
    "#norm = mlt.colors.Normalize(vmin=0.,vmax=1.)\n",
    "layer = tf.keras.layers.LayerNormalization(axis=1)\n",
    "fig=plt.figure(figsize=(18,30))\n",
    "for i in range(5):\n",
    "    while (not ready):\n",
    "        rand = random.randint(0, len(train_y)-1)\n",
    "        if np.array_equal(train_y[rand],[0, 1.0, 0.]):\n",
    "            plt.subplot(1,5,i+1)\n",
    "            plt.subplot(1,5,i+1)\n",
    "            \n",
    "            output = layer(train_X[rand])\n",
    "            plt.imshow(output)\n",
    "            #plt.imshow(train_X[rand], norm=norm)\n",
    "            plt.title(str(rand)+' / '+str(train_y[rand]))\n",
    "            ready = True\n",
    "    ready=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_X[index], norm=norm)\n",
    "#norm = mlt.colors.Normalize(vmin=0.,vmax=1.)\n",
    "layer = tf.keras.layers.LayerNormalization(axis=1)\n",
    "fig=plt.figure(figsize=(18,30))\n",
    "for i in range(5):\n",
    "    while (not ready):\n",
    "        rand = random.randint(0, len(train_y)-1)\n",
    "        if np.array_equal(train_y[rand],[0., 0., 1.]):\n",
    "            plt.subplot(1,5,i+1)\n",
    "            plt.subplot(1,5,i+1)\n",
    "            \n",
    "            output = layer(train_X[rand])\n",
    "            plt.imshow(output)\n",
    "            #plt.imshow(train_X[rand], norm=norm)\n",
    "            plt.title(str(rand)+' / '+str(train_y[rand]))\n",
    "            ready = True\n",
    "    ready=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "layer = tf.keras.layers.LayerNormalization(axis=1)\n",
    "for x in train_X:\n",
    "    fig=plt.figure(figsize=(18,30))\n",
    "    output = layer(x)\n",
    "    plt.imshow(output)\n",
    "    plt.title(str(i))\n",
    "    if (train_y[i]==1.0):\n",
    "        fig.savefig('train/up/'+str(i)+'.png')\n",
    "    elif (train_y[i]==-1.0):\n",
    "        fig.savefig('train/down/'+str(i)+'.png')\n",
    "    elif (train_y[i]==0):\n",
    "        fig.savefig('train/const/'+str(i)+'.png')\n",
    "    plt.close(fig)\n",
    "    i+=1\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for x in test_X:\n",
    "    fig=plt.figure(figsize=(18,30))\n",
    "    output = layer(x)\n",
    "    plt.imshow(output)\n",
    "    plt.title(str(i))\n",
    "    if (test_y[i]==1.0):\n",
    "        fig.savefig('test/up/'+str(i)+'.png')\n",
    "    elif (test_y[i]==-1.0):\n",
    "        fig.savefig('test/down/'+str(i)+'.png')\n",
    "    elif (test_y[i]==0):\n",
    "        fig.savefig('test/const/'+str(i)+'.png')\n",
    "    plt.close(fig)\n",
    "    i+=1\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3=tf.keras.layers.LayerNormalization()\n",
    "output = layer3(train_X[837])\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = myCallback()\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.6):\n",
    "            print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "#callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "def scheduler(epoch, lr):\n",
    "      if epoch < 10:\n",
    "        return lr\n",
    "      else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variation_calc(a_matrix, Weight):\n",
    "    line_lenght=(a_matrix.shape)[-2]-1\n",
    "    variation_matrix=[]\n",
    "    while line_lenght>0:\n",
    "        i=0\n",
    "        variation_line=[]\n",
    "        for x in a_matrix[line_lenght]:\n",
    "            var= (((abs(a_matrix[line_lenght][i])/abs(a_matrix[line_lenght-1][i]))-1)) * Weight\n",
    "            variation_line.append(var)\n",
    "            i+=1\n",
    "        variation_matrix.append(variation_line)\n",
    "        line_lenght-=1\n",
    "    \n",
    "    variation_matrix = a_matrix[1:]+(a_matrix[1:]*(variation_matrix))\n",
    "    return variation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.layers.Activation('relu')\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(train_X[0].shape)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]))\n",
    "model.add(tf.keras.layers.SimpleRNN(100, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dropout(0.6))\n",
    "model.add(tf.keras.layers.SimpleRNN(100))\n",
    "model.add(tf.keras.layers.Dropout(0.8))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(100,kernel_regularizer=tf.keras.regularizers.L1(0.001),activity_regularizer=tf.keras.regularizers.L2(0.01)))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]))\n",
    "model.add(tf.keras.layers.SimpleRNN(50, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dropout(0.6))\n",
    "model.add(tf.keras.layers.SimpleRNN(50))\n",
    "model.add(tf.keras.layers.Dropout(0.8))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(32,kernel_regularizer=tf.keras.regularizers.L1(0.001),activity_regularizer=tf.keras.regularizers.L2(0.01)))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-3, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LayerNormalization(input_shape=(train_X[0].shape)))\n",
    "\n",
    "\n",
    "#model.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "#model.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]))\n",
    "model.add(tf.keras.layers.LSTM(100, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(32))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(64,kernel_regularizer=tf.keras.regularizers.L1(0.001),activity_regularizer=tf.keras.regularizers.L2(0.01)))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(3, activation= 'softmax'))\n",
    "\n",
    "#model.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=-1),input_shape=[None]))\n",
    "#model.add(tf.keras.layers.SimpleRNN(100))\n",
    "#model.add(tf.keras.layers.Dense(1, activation= 'relu'))\n",
    "#optimizer = 'adam'\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=[tf.keras.metrics.CategoricalCrossentropy(),\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback1 = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "history = model.fit(train_X, train_y, epochs=100,batch_size=32, validation_data=(test_X, test_y),verbose=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sampling model loss: 0.3165 - categorical_crossentropy: 0.2438 - accuracy: 0.9091 - val_loss: 0.7229 - val_categorical_crossentropy: 0.6535 - val_accuracy: 0.7582\n",
    "model.save('models/LSTMM10609-class-015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'][:], label='train')\n",
    "plt.plot(history.history['val_accuracy'][:], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'][:], label='train')\n",
    "plt.plot(history.history['val_loss'][:], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = thread_training.thread_training(model, train_X, train_y, 1000, 72, (test_X, test_y), 1, False) \n",
    "t1_history = t1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = thread_training.thread_training(model, train_X, train_y, 1000, 72, (test_X, test_y), 1, False) \n",
    "t2_history = t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('models/LSTH1609-class-007')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = train_X_os\n",
    "temp_y = train_y_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_evaluate = x_processed\n",
    "y_evaluate = y_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_evaluate, y_evaluate, batch_size=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = yhat[:100]\n",
    "ts = test_y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "#dlm.plot_series(np.arange(len(ps)), ps[:])\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(len(ps)), [x[0] for x in ps[:]])\n",
    "#plt.subplot(212)\n",
    "plt.plot(np.arange(len(ts)), [x[0] for x in ts[:]])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "#dlm.plot_series(np.arange(len(ps)), ps[:])\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(len(ps)),ps)\n",
    "#plt.subplot(212)\n",
    "plt.plot(np.arange(len(ts)),ts)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('shutdown -s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = tml.thread_main_loop(model, 60, 9, 0, \"0\", \"1\",1)\n",
    "t1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.kill()\n",
    "t1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.reshape(np.array(x), [None, 16,16,2,1]),input_shape=[None]),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(104, activation='softmax')\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
